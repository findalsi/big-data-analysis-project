{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fb3ee5",
   "metadata": {},
   "source": [
    "# üìò Final Project: Data Analysis using MongoDB and Apache Spark\n",
    "\n",
    "This project demonstrates how to work with a real-world dataset (Amazon Books Reviews Dataset) using MongoDB for storage and Apache Spark (PySpark) for processing. We explore schema design, querying, performance optimization, and visual insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f56ae",
   "metadata": {},
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df144e86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Configure Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb67d72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Load CSV files\n",
    "books_df = pd.read_csv(\"books_data.csv\")\n",
    "ratings_df = pd.read_csv(\"Books_rating.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bd1d8",
   "metadata": {},
   "source": [
    "## üìÇ Dataset Overview\n",
    "\n",
    "We use the **Amazon Books Reviews** dataset from Kaggle, which contains user-generated reviews for a wide variety of books available on Amazon. This real-world dataset includes valuable attributes such as:\n",
    "\n",
    "- **Title**: The name of the book.\n",
    "- **Author(s)**: The author(s) of the book.\n",
    "- **Categories**: Genre or subject of the book (e.g., Comics, Fiction, Education).\n",
    "- **Rating**: User rating scores, typically from 1 to 5.\n",
    "- **Review Text**: Actual written review provided by users.\n",
    "- **Review Date**: When the review was posted.\n",
    "- **ASIN**: Unique Amazon product identifier for each book.\n",
    "\n",
    "This dataset provides an excellent foundation for exploring data storage, processing, and analysis using MongoDB and Apache Spark due to its unstructured nature and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0d1a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Basic exploration\n",
    "print(\"üìò Books Data Sample:\")\n",
    "print(books_df.head(), \"\\n\")\n",
    "\n",
    "print(\"üìù Ratings Data Sample:\")\n",
    "print(ratings_df.head(), \"\\n\")\n",
    "\n",
    "print(\"üìä Shapes:\")\n",
    "print(\"Books Data Shape:\", books_df.shape)\n",
    "print(\"Ratings Data Shape:\", ratings_df.shape, \"\\n\")\n",
    "\n",
    "print(\"üîç Missing Values in Books Data:\")\n",
    "print(books_df.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"üîç Missing Values in Ratings Data:\")\n",
    "print(ratings_df.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02a937",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Optional Cleaning\n",
    "# Drop rows with missing essential review text\n",
    "ratings_df = ratings_df.dropna(subset=[\"review/text\"])\n",
    "\n",
    "\n",
    "# Fill missing summaries with a placeholder (safe version)\n",
    "ratings_df[\"review/summary\"] = ratings_df[\"review/summary\"].fillna(\"No summary provided\")\n",
    "\n",
    "\n",
    "# You can apply similar cleaning to books_df if needed\n",
    "# books_df = books_df.dropna()  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa326a4e",
   "metadata": {},
   "source": [
    "## üóÉ Storing Dataset in MongoDB\n",
    "We connect to MongoDB and insert our dataset using an optimized¬†schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc2d55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")  # Change if hosted elsewhere\n",
    "db = client[\"books_database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e86966",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Convert DataFrames to list of dictionaries\n",
    "books_data = books_df.to_dict(\"records\")\n",
    "ratings_data = ratings_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652ef84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "def insert_in_batches(collection, data, batch_size=100):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        try:\n",
    "            collection.insert_many(batch)\n",
    "        except BulkWriteError as bwe:\n",
    "            print(f\"‚ùå Bulk write error: {bwe.details}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inserting batch {i // batch_size}: {e}\")\n",
    "\n",
    "db[\"books\"].drop()\n",
    "db[\"ratings\"].drop()\n",
    "\n",
    "db[\"books\"].insert_many(books_data)\n",
    "insert_in_batches(db[\"ratings\"], ratings_data)\n",
    "\n",
    "print(\"‚úÖ Data inserted into MongoDB¬†successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a344e6",
   "metadata": {},
   "source": [
    "## ‚öô Data Processing with PySpark\n",
    "We use PySpark to read, transform, and analyze the dataset loaded from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e1e2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BooksMiniProject\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load your CSV files into Spark DataFrames\n",
    "books_df = spark.read.csv(\"books_data.csv\", header=True, inferSchema=True)\n",
    "ratings_df = spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Optional: Show first few rows for confirmation\n",
    "books_df.show(5)\n",
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6de1d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read documents from MongoDB, exclude '_id'\n",
    "books_docs = list(db[\"books\"].find({}, {\"_id\": 0}))\n",
    "ratings_docs = list(db[\"ratings\"].find({}, {\"_id\": 0}).limit(10000))  # üîÅ Adjust limit as needed\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "df_books = spark.createDataFrame(books_docs)\n",
    "df_ratings = spark.createDataFrame(ratings_docs)\n",
    "\n",
    "# Show sample rows\n",
    "df_books.show(3)\n",
    "df_ratings.show(3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
