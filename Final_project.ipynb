{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fb3ee5",
   "metadata": {},
   "source": [
    "# üìò Final Project: Data Analysis using MongoDB and Apache Spark\n",
    "\n",
    "This project demonstrates how to work with a real-world dataset (Amazon Books Reviews Dataset) using MongoDB for storage and Apache Spark (PySpark) for processing. We explore schema design, querying, performance optimization, and visual insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f56ae",
   "metadata": {},
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df144e86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Configure Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb67d72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Load CSV files\n",
    "books_df = pd.read_csv(\"books_data.csv\")\n",
    "ratings_df = pd.read_csv(\"Books_rating.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bd1d8",
   "metadata": {},
   "source": [
    "## üìÇ Dataset Overview\n",
    "\n",
    "We use the **Amazon Books Reviews** dataset from Kaggle, which contains user-generated reviews for a wide variety of books available on Amazon. This real-world dataset includes valuable attributes such as:\n",
    "\n",
    "- **Title**: The name of the book.\n",
    "- **Author(s)**: The author(s) of the book.\n",
    "- **Categories**: Genre or subject of the book (e.g., Comics, Fiction, Education).\n",
    "- **Rating**: User rating scores, typically from 1 to 5.\n",
    "- **Review Text**: Actual written review provided by users.\n",
    "- **Review Date**: When the review was posted.\n",
    "- **ASIN**: Unique Amazon product identifier for each book.\n",
    "\n",
    "This dataset provides an excellent foundation for exploring data storage, processing, and analysis using MongoDB and Apache Spark due to its unstructured nature and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0d1a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Basic exploration\n",
    "print(\"üìò Books Data Sample:\")\n",
    "print(books_df.head(), \"\\n\")\n",
    "\n",
    "print(\"üìù Ratings Data Sample:\")\n",
    "print(ratings_df.head(), \"\\n\")\n",
    "\n",
    "print(\"üìä Shapes:\")\n",
    "print(\"Books Data Shape:\", books_df.shape)\n",
    "print(\"Ratings Data Shape:\", ratings_df.shape, \"\\n\")\n",
    "\n",
    "print(\"üîç Missing Values in Books Data:\")\n",
    "print(books_df.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"üîç Missing Values in Ratings Data:\")\n",
    "print(ratings_df.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02a937",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Optional Cleaning\n",
    "# Drop rows with missing essential review text\n",
    "ratings_df = ratings_df.dropna(subset=[\"review/text\"])\n",
    "\n",
    "\n",
    "# Fill missing summaries with a placeholder (safe version)\n",
    "ratings_df[\"review/summary\"] = ratings_df[\"review/summary\"].fillna(\"No summary provided\")\n",
    "\n",
    "\n",
    "# You can apply similar cleaning to books_df if needed\n",
    "# books_df = books_df.dropna()  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa326a4e",
   "metadata": {},
   "source": [
    "## üóÉ Storing Dataset in MongoDB\n",
    "We connect to MongoDB and insert our dataset using an optimized¬†schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc2d55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")  # Change if hosted elsewhere\n",
    "db = client[\"books_database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e86966",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Convert DataFrames to list of dictionaries\n",
    "books_data = books_df.to_dict(\"records\")\n",
    "ratings_data = ratings_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652ef84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "def insert_in_batches(collection, data, batch_size=100):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        try:\n",
    "            collection.insert_many(batch)\n",
    "        except BulkWriteError as bwe:\n",
    "            print(f\"‚ùå Bulk write error: {bwe.details}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inserting batch {i // batch_size}: {e}\")\n",
    "\n",
    "db[\"books\"].drop()\n",
    "db[\"ratings\"].drop()\n",
    "\n",
    "db[\"books\"].insert_many(books_data)\n",
    "insert_in_batches(db[\"ratings\"], ratings_data)\n",
    "\n",
    "print(\"‚úÖ Data inserted into MongoDB¬†successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a344e6",
   "metadata": {},
   "source": [
    "## ‚öô Data Processing with PySpark\n",
    "We use PySpark to read, transform, and analyze the dataset loaded from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e1e2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BooksMiniProject\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load your CSV files into Spark DataFrames\n",
    "books_df = spark.read.csv(\"books_data.csv\", header=True, inferSchema=True)\n",
    "ratings_df = spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Optional: Show first few rows for confirmation\n",
    "books_df.show(5)\n",
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6de1d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read documents from MongoDB, exclude '_id'\n",
    "books_docs = list(db[\"books\"].find({}, {\"_id\": 0}))\n",
    "ratings_docs = list(db[\"ratings\"].find({}, {\"_id\": 0}).limit(10000))  # üîÅ Adjust limit as needed\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "df_books = spark.createDataFrame(books_docs)\n",
    "df_ratings = spark.createDataFrame(ratings_docs)\n",
    "\n",
    "# Show sample rows\n",
    "df_books.show(3)\n",
    "df_ratings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5adc6",
   "metadata": {},
   "source": [
    "## üîç Querying with Spark SQL\n",
    "Analyze average ratings using Spark SQL and display top-rated books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560de860",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Optional: rename to avoid column name conflict during join\n",
    "df_ratings = df_ratings.withColumnRenamed(\"review/score\", \"rating_score\")\n",
    "\n",
    "# Perform inner join on Title\n",
    "df_joined = df_ratings.join(df_books, on=\"Title\", how=\"inner\")\n",
    "\n",
    "# Select relevant columns\n",
    "df_selected = df_joined.select(\n",
    "    \"Title\",\n",
    "    \"authors\",\n",
    "    \"categories\",\n",
    "    \"rating_score\"\n",
    ")\n",
    "\n",
    "# Compute average rating per book\n",
    "df_avg = df_selected.groupBy(\"Title\", \"authors\", \"categories\") \\\n",
    "    .agg(avg(\"rating_score\").alias(\"avg_rating\")) \\\n",
    "    .orderBy(\"avg_rating\", ascending=False)\n",
    "\n",
    "# Show top 10 rated books\n",
    "df_avg.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12250aab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get top 10 books from df_avg\n",
    "top_books = df_avg.limit(10).toPandas().to_dict(\"records\")\n",
    "\n",
    "# Save to MongoDB\n",
    "db[\"top_books\"].drop()  # Drop existing collection if it exists\n",
    "db[\"top_books\"].insert_many(top_books)  # Insert top books\n",
    "\n",
    "print(\"‚úÖ Top 10 books saved to MongoDB collection 'top_books'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965504ef",
   "metadata": {},
   "source": [
    "## üìä Genre-Based Filtering\n",
    "We filter and analyze specific genres like Comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236f6b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filter books where categories contain \"Comics\"\n",
    "df_comics = df_avg.filter(df_avg.categories.contains(\"Comics\"))\n",
    "df_comics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d4841",
   "metadata": {},
   "source": [
    "### Indexing for Performance\n",
    "\n",
    "Create indexes on frequently used fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdddf47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "db[\"books\"].create_index(\"Title\")\n",
    "db[\"books\"].create_index(\"publishedDate\")\n",
    "db[\"books\"].create_index(\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f64bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "list(db[\"books\"].list_indexes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06fe030",
   "metadata": {},
   "source": [
    "## üìà Visualization of Top-Rated Books\n",
    "Using Matplotlib to show top-rated books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819a255",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to Pandas DataFrame from Spark DataFrame\n",
    "top_books_df = df_avg.limit(10).toPandas()\n",
    "\n",
    "# Check if the DataFrame is not empty and contains required columns\n",
    "if not top_books_df.empty and 'Title' in top_books_df.columns and 'avg_rating' in top_books_df.columns:\n",
    "    # Plot horizontal bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_books_df['Title'], top_books_df['avg_rating'], color='skyblue')\n",
    "    plt.xlabel(\"Average Rating\")\n",
    "    plt.title(\"Top 10 Rated Books\")\n",
    "    plt.gca().invert_yaxis()  # Puts highest-rated book at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† DataFrame is empty or missing required columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f7e9d",
   "metadata": {},
   "source": [
    "## üìÜ Year-Based Analysis\n",
    "Analyzing books by year of¬†publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810082e",
   "metadata": {},
   "source": [
    "Filter by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270afb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Join df_avg with df_books to access publishedDate\n",
    "df_avg_with_date = df_avg.join(df_books.select('Title', 'publishedDate'), on='Title', how='inner')\n",
    "\n",
    "# Filter books published after 2010\n",
    "df_recent_books = df_avg_with_date.filter(df_avg_with_date.publishedDate >= '2010')\n",
    "\n",
    "# Show result\n",
    "df_recent_books.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
